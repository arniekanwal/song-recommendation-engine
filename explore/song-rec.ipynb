{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any necessary libraries / modules\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import pandas as pd # we only use pandas as examples to compare to\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/tracks_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with PySpark: 2.1501 seconds\n",
      "Time taken with Pandas: 4.6297 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a path to the dataset and read into our Spark Session\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"datasets\", \"tracks_features.csv\")\n",
    "print(DATA_PATH)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SongRecommendation\").getOrCreate()\n",
    "\n",
    "'''\n",
    "First exercise is to just compare execution time between Pandas and PySpark...\n",
    "\n",
    "There is already a meaningful difference due to dataset size of 1.2million rows, but remember \n",
    "that Spark is being run locally (single cluster) and our dataset is still relatively small \n",
    "(i.e. 350mb vs hundreds of gigs or tb in real-world applications)\n",
    "'''\n",
    "# Function to read with PySpark\n",
    "def read_with_spark():\n",
    "    df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# Function to read with Pandas\n",
    "def read_with_pandas():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Measure execution time\n",
    "spark_time = timeit.timeit(read_with_spark, number=1)  # Run once\n",
    "pandas_time = timeit.timeit(read_with_pandas, number=1)  # Run once\n",
    "\n",
    "# Print results\n",
    "print(f\"Time taken with PySpark: {spark_time:.4f} seconds\")\n",
    "print(f\"Time taken with Pandas: {pandas_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ready our data in memory and get ready for data processingx\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
